{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b89e295",
   "metadata": {},
   "source": [
    "# 1. Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "429f299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from scipy import stats\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class BaggingDecisionTree:\n",
    "    def __init__(self, n_estimators=10, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "        for i in range(n_estimators):\n",
    "            learner = DecisionTreeClassifier(max_depth=self.max_depth, random_state=42)\n",
    "            self.estimators.append(learner)\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        for learner in self.estimators:\n",
    "            if self.n_estimators > 1:\n",
    "                inds = np.random.randint(0,len(X_train), size=300+int(len(X_train)/self.n_estimators))\n",
    "                X_train_sub = X_train[inds,:]\n",
    "                y_train_sub = y_train[inds]\n",
    "                #print(f'ind_1={inds[0]}')\n",
    "                learner.fit(X_train_sub, y_train_sub)\n",
    "            else:\n",
    "                #print('Fitting single learning to entire training data')\n",
    "                learner.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        results = []\n",
    "        for i,learner in enumerate(self.estimators):\n",
    "            res = learner.predict(X_test)\n",
    "            results.append(res)\n",
    "        \n",
    "        results = np.array(results)\n",
    "        results_df = pd.DataFrame(results.T)\n",
    "        results_df.to_csv('weak_learner_results.csv', index=False)\n",
    "        return np.round(np.mean(results,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "99f4451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "cars = pd.read_csv(\"car_evaluation.csv\")\n",
    "#print(cars.describe())\n",
    "\n",
    "# Work on non-numeric data points\n",
    "cars['number of doors'] = cars['number of doors'].replace('5more', '5')\n",
    "cars['number of doors'] = cars['number of doors'].astype('float')\n",
    "cars['number of persons'] = cars['number of persons'].replace('more', '5')\n",
    "cars['number of persons'] = cars['number of persons'].astype('float')\n",
    "#print(f'cars.shape={cars.shape}')\n",
    "#print(cars['buying price'].unique())\n",
    "#print(cars['maintenance cost'].unique())\n",
    "#print(cars['lug_boot'].unique())\n",
    "#print(cars['safety'].unique())\n",
    "#print(cars['decision'].unique())\n",
    "#print(cars.head())\n",
    "\n",
    "# use encoding on categorical features\n",
    "enc1 = OrdinalEncoder(categories=[['vhigh', 'high', 'med', 'low']])\n",
    "enc2 = OrdinalEncoder(categories=[['big', 'med', 'small']])\n",
    "enc3 = OrdinalEncoder(categories=[['vgood', 'good', 'acc', 'unacc']])\n",
    "cars['bp_enc'] = enc1.fit_transform(cars[['buying price']])\n",
    "cars['mc_enc'] = enc1.fit_transform(cars[['maintenance cost']])\n",
    "cars['lb_enc'] = enc2.fit_transform(cars[['lug_boot']])\n",
    "cars['sf_enc'] = enc1.fit_transform(cars[['safety']])\n",
    "cars['dec_enc'] = enc3.fit_transform(cars[['decision']])\n",
    "cars = cars.drop(['buying price', 'maintenance cost', 'lug_boot', 'safety', 'decision'], axis=1)\n",
    "cars = cars.reset_index(drop=True)\n",
    "#print(cars.head())\n",
    "#print(cars.describe())\n",
    "cars = cars.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "31813b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=1, max_depth=3, accuracy=0.7977, r2=-0.7880, rms=0.5613\n",
      "n_estimators=10, max_depth=5, accuracy=0.8873, r2=0.7272, rms=0.3914\n",
      "When we use n_estimators=1 (single tree for the entire dataset), the performance is not very good.\n",
      "As we increase the number of estimators and the depth of the tree, the performance of the ensemble improves.\n"
     ]
    }
   ],
   "source": [
    "## Train and test the decision trees\n",
    "\n",
    "#print(cars.describe())\n",
    "np.random.seed = 42\n",
    "X = cars[:,:6]\n",
    "#print(f'{X.shape}, {np.unique(X[:,0])}, {np.unique(X[:,1])}, {np.unique(X[:,2])}, {np.unique(X[:,3])}, {np.unique(X[:,4])}, {np.unique(X[:,5])}')\n",
    "y = cars[:,6]\n",
    "#print(f'{y.shape}, {np.unique(y)}')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, stratify=y, random_state=42)\n",
    "\n",
    "#print(f'X_train.shape={X_train.shape}')\n",
    "\n",
    "# \n",
    "\n",
    "# Try different configurations of trees\n",
    "for n_est, dep in zip([1, 10],[3, 5]):\n",
    "    tree = BaggingDecisionTree(n_estimators=n_est, max_depth=dep)\n",
    "    tree.train(X_train, y_train)\n",
    "    results = tree.predict(X_test)\n",
    "    accuracy = accuracy_score(results, y_test)\n",
    "    r2_score1 = r2_score(results, y_test)\n",
    "    rms_error = np.sqrt(mean_squared_error(results, y_test))\n",
    "    print(f'n_estimators={n_est}, max_depth={dep}, accuracy={accuracy:.4f}, r2={r2_score1:.4f}, rms={rms_error:.4f}')\n",
    "\n",
    "print(f'When we use n_estimators=1 (single tree for the entire dataset), the performance is not very good.')\n",
    "print('As we increase the number of estimators and the depth of the tree, the performance of the ensemble improves.')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb3b98",
   "metadata": {},
   "source": [
    "# 2. Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "756d86ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BoostedDecisionTree:\n",
    "    def __init__(self, n_estimators=10, max_depth=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "        self.alphas = []\n",
    "        self.num_classes = None\n",
    "        for i in range(n_estimators):\n",
    "            learner = DecisionTreeClassifier(max_depth=self.max_depth, random_state=42)\n",
    "            self.estimators.append(learner)\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        self.num_classes = len(np.unique(y_train))\n",
    "        weights = np.ones_like(y_train)\n",
    "        weights /= len(y_train)\n",
    "        for learner in self.estimators:    \n",
    "            learner.fit(X_train, y_train, sample_weight=weights)\n",
    "            y_pred = learner.predict(X_train)\n",
    "            errors = (y_pred != y_train).astype(int)\n",
    "            #print(f'y_pred={y_pred[10:20]}')\n",
    "            #print(f'y_train={y_train[10:20]}')\n",
    "            #print(f'errors={errors[10:20]}')\n",
    "            epsilon_t = np.sum(weights*errors)/np.sum(weights)\n",
    "            alpha_t = 0.5*np.log((1-epsilon_t)/epsilon_t) + np.log(3)\n",
    "            self.alphas.append(alpha_t)\n",
    "            weights *= (1 + errors * np.exp(alpha_t))\n",
    "            weights /= np.sum(weights)\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        results = np.zeros(shape=(len(X_test), self.num_classes))\n",
    "        \n",
    "        for l, learner in enumerate(self.estimators):\n",
    "            res = learner.predict(X_test)\n",
    "            for i in range(len(X_test)):\n",
    "                results[i, int(res[i])] += self.alphas[l]\n",
    "        \n",
    "        results_df = pd.DataFrame(results.T)\n",
    "        results_df.to_csv('weak_learner_results.csv', index=False)\n",
    "        return np.argmax(results, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "e0471c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=10, max_depth=1, accuracy=0.8006, r2=-1.0284, rms=0.5840\n",
      "n_estimators=20, max_depth=1, accuracy=0.7775, r2=0.3862, rms=0.5638\n",
      "n_estimators=20, max_depth=3, accuracy=0.9711, r2=0.9161, rms=0.2150\n",
      "As seen in the scores above, as we increase the number of estimators and the depth of trees, the accuracy of the ensemble improves\n"
     ]
    }
   ],
   "source": [
    "#print(cars.describe())\n",
    "np.random.seed = 42\n",
    "X = cars[:,:6]\n",
    "#print(f'{X.shape}, {np.unique(X[:,0])}, {np.unique(X[:,1])}, {np.unique(X[:,2])}, {np.unique(X[:,3])}, {np.unique(X[:,4])}, {np.unique(X[:,5])}')\n",
    "y = cars[:,6]\n",
    "#print(f'{y.shape}, {np.unique(y)}')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.8, stratify=y, random_state=42)\n",
    "\n",
    "#print(f'X_train.shape={X_train.shape}')\n",
    "\n",
    "# \n",
    "for n_est, dep in zip([10, 20, 20],[1, 1, 3]):\n",
    "    tree = BoostedDecisionTree(n_estimators=n_est, max_depth=dep)\n",
    "    tree.train(X_train, y_train)\n",
    "    results = tree.predict(X_test)\n",
    "    accuracy = accuracy_score(results, y_test)\n",
    "    r2_score1 = r2_score(results, y_test)\n",
    "    rms_error = np.sqrt(mean_squared_error(results, y_test))\n",
    "    print(f'n_estimators={n_est}, max_depth={dep}, accuracy={accuracy:.4f}, r2={r2_score1:.4f}, rms={rms_error:.4f}')\n",
    "\n",
    "print('As seen in the scores above, as we increase the number of estimators and the depth of trees, the accuracy of the ensemble improves')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
